package tClassifier;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;



/*
 *		Class TwitterClassifier
 * 
 *		@desc Classifier of tweets, using MapResuduce, Mahout and a Naive Bayes model.
 *			  Process inputs:
 *					-Naive Bayes model trained in Mahout.
 *					-Document Frequency of Training Set.
 *					-Dictionnary of Training Set.
 *					-Sequence File with tweets to classify.
 *
 *		@author Vicente Ruben Del Pino Ruiz <<ruben.delpino@gmail.com>>
 *
 */
public class TwitterClassifier {

	public static class ClassifierMap extends Mapper<Text, Text, Text, Text> {
		
//		private static Classifier classifier;
		
		
		/*
		 * 		protected void setup
		 * 
		 * 		@desc Setups the Mapper creating the classifier in case that is not created yet.
		 *
		 */	
		
		@Override
		protected void setup(Context context) throws IOException {
//			initClassifier(context);
		}
		
		/*
		 * 		public void initClassifier
		 * 
		 * 		@desc Inits the classifier.
		 * 
		 * 		@param Context context. Context that will be used for extracting the configuration.
		 * 		
		 */	
		
/*		
  		private static void initClassifier(Context context) throws IOException {
			if (classifier == null) {
				synchronized (ClassifierMap.class) {
					if (classifier == null) {
						classifier = new Classifier(context.getConfiguration());
						}
				}
			}
		}
*/
		 
		/*
		 * 		public void map
		 * 
		 * 		@desc Map for MapReduce framework. Takes from each line the key (TweetID) and the value (Tweet).
		 * 
		 * 		@param Text key. Tweet ID 
		 * 		@param Text value.		Tweet related to Tweet ID
		 * 		@param Context.		    Context of the map reduce
		 * 
		 * 		@return (via context) TweetID and Category
		 * 
		 */	
		public void map(Text key, Text value, Context context) throws IOException, InterruptedException {
		

			//int bestCategoryId =classifier.classify(value);
			//Text category = new Text(Integer.toString(bestCategoryId));
			//context.write(key, category);
		
			Text k = new Text("1");
			Text category = new Text("2");
			context.write(k, category);

		}
			
	}

	
	
	
	@SuppressWarnings("deprecation")
	public static void main(String[] args) throws Exception {
		
		//Filter the input to the process
		if (args.length < 3) {
			System.out.println("Arguments: [model] [documentFrequency] [dictionary] [tweet file] [output directory]");
			return;
		}
		
		//Read the parameters in the command line
		String modelPath = args[0];
		String documentFrequencyPath = args[1];
		String dictionaryPath = args[2];
		String tweetsPath = args[3];
		String outputPath = args[4];

		//Setup the configuration for the MapReduce
		Configuration conf = new Configuration();

        conf.setStrings(Classifier.MODEL_PATH_CONF, modelPath);
        conf.setStrings(Classifier.DICTIONARY_PATH_CONF, dictionaryPath);
        conf.setStrings(Classifier.DOCUMENT_FREQUENCY_PATH_CONF, documentFrequencyPath);
        
        conf.set("mapreduce.map.memory.mb","3000");
        conf.set("mapreduce.map.java.opts","-Xmx2024m");
        conf.set("mapreduce.task.io.sort.mb", "1000");
        
        //Configure the job to be executed
		Job job = new Job(conf, "TwitterClassifier");

		job.setJarByClass(TwitterClassifier.class);
		
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);
		job.setMapperClass(ClassifierMap.class);

		job.setInputFormatClass(KeyValueTextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		FileInputFormat.addInputPath(job, new Path(tweetsPath));
		FileOutputFormat.setOutputPath(job, new Path(outputPath));

		//Execute the job
		job.waitForCompletion(true);
	}
}