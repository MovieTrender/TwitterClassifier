package tClassifier;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.mahout.classifier.AbstractVectorClassifier;
import org.apache.mahout.classifier.naivebayes.NaiveBayesModel;
import org.apache.mahout.classifier.naivebayes.StandardNaiveBayesClassifier;
import org.apache.mahout.common.Pair;
import org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable;
import org.apache.mahout.math.RandomAccessSparseVector;
import org.apache.mahout.math.Vector;
import org.apache.mahout.math.Vector.Element;
import org.apache.mahout.math.VectorWritable;
import org.apache.mahout.vectorizer.TFIDF;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;

import com.google.common.collect.ConcurrentHashMultiset;
import com.google.common.collect.Multiset;

public class Classifier {

	
	private static AbstractVectorClassifier classifier;
	private static NaiveBayesModel naiveBayesModel;
	private static Map<String, Integer> dictionary;
	private static Map<Integer, Long> documentFrequency;
	
	
	public final static String DICTIONARY_PATH_CONF = "dictionaryPath";
	public final static String DOCUMENT_FREQUENCY_PATH_CONF = "documentFrequencyPath";
	public final static String MODEL_PATH_CONF = "modelPath";
	
	public Classifier(Configuration configuration) throws IOException {
		
		 String modelPath = configuration.getStrings(MODEL_PATH_CONF)[0];
	     String dictionaryPath = configuration.getStrings(DICTIONARY_PATH_CONF)[0];
	     String documentFrequencyPath = configuration.getStrings(DOCUMENT_FREQUENCY_PATH_CONF)[0];
	 
	     dictionary = readDictionnary(configuration, new Path(dictionaryPath));
	     documentFrequency = readDocumentFrequency(configuration, new Path(documentFrequencyPath));
	
		 naiveBayesModel= NaiveBayesModel.materialize(new Path(modelPath),configuration);
		 classifier = new StandardNaiveBayesClassifier(naiveBayesModel);

	
	}

	public int classify(Text text) throws IOException {
		

	      int documentCount = documentFrequency.get(-1).intValue();
	      HashMap<String, Integer> words = new HashMap<String, Integer>();
	 
	      // extract words from tweet
	      
	      String delims = "[ ]+";
	      String [] ts = text.toString().split(delims);
	      
	      int wordCount = 0;
	      
	      for (int i=0;i<ts.length;i++){
	    	  Integer wordId = dictionary.get(ts[i]);
              // if the word is not in the dictionary, skip it
              if (wordId != null) {
            	
            	  if(!words.containsKey(ts[i])){ 
                      words.put(ts[i], 1); 
                  }else{
                      int countWord = words.get(ts[i]) + 1; 
                      words.remove(ts[i]);
                      words.put(ts[i], countWord); 
                  }

                  wordCount++;
              }
	      }
	       
	 
	      // create vector wordId => weight using tfidf
	      Vector vector = new RandomAccessSparseVector(10000);
	      TFIDF tfidf = new TFIDF();
	      
	        for (Multiset.Entry entry: words.entrySet() ) {
	            String word = entry.getElement();
	            int count = entry.getCount();
	            Integer wordId = dictionary.get(word);
	            Long freq = documentFrequency.get(wordId);
	            double tfIdfValue = tfidf.calculate(count, freq.intValue(), wordCount, documentCount);
	            vector.setQuick(wordId, tfIdfValue);
	        }
		
		
		 Vector result = classifier.classifyFull(vector);
	     
	     double bestScore = -Double.MAX_VALUE;
	     int bestCategoryId = -1;
	        for(Element element: result) {
	            int categoryId = element.index();
	            double score = element.get();
	            if (score > bestScore) {
	                bestScore = score;
	                bestCategoryId = categoryId;
	            }
	        }
	 
	     return bestCategoryId;
	     

		
	}
	
	 private static Map<String, Integer> readDictionnary(Configuration conf, Path dictionnaryPath) {
	        Map<String, Integer> dictionnary = new HashMap<String, Integer>();
	        for (Pair<Text, IntWritable> pair : new SequenceFileIterable<Text, IntWritable>(dictionnaryPath, true, conf)) {
	            dictionnary.put(pair.getFirst().toString(), pair.getSecond().get());
	        }
	        return dictionnary;
	    }
	 
	    private static Map<Integer, Long> readDocumentFrequency(Configuration conf, Path documentFrequencyPath) {
	        Map<Integer, Long> documentFrequency = new HashMap<Integer, Long>();
	        for (Pair<IntWritable, LongWritable> pair : new SequenceFileIterable<IntWritable, LongWritable>(documentFrequencyPath, true, conf)) {
	            documentFrequency.put(pair.getFirst().get(), pair.getSecond().get());
	        }
	        return documentFrequency;
	    }
	
	
	


}